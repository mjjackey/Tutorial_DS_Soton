{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Hadoop and Hadoop Distributed File System (HDFS)\n",
    "\n",
    "In this tutorial, you will:\n",
    "\n",
    "* Create a MapReduce task using the Hadoop \"Streaming\" API (Python)\n",
    "* Import a spam dataset into the Hadoop Distributed File System (HDFS)\n",
    "* Run a MapReduce task using HDFS\n",
    "\n",
    "## Setup\n",
    "* This tutorial expects you to be using the COMP6235 Virtual Machine for VirtualBox. No support is provided for other solutions. Setup instructions are available at http://edshare.soton.ac.uk/id/document/324163\n",
    "* Run \"run-jupyter\" to start Jupyter Notebook\n",
    "* Download the .ipynb file at http://edshare.soton.ac.uk/19650/ and import it into Jupyter\n",
    "\n",
    "# Refresher: What is Hadoop?\n",
    "\n",
    "Apache Hadoop is an open-source software framework for distributing the processing of large amounts of data across multiple machines. It has an emphasis on fault-tolerant processing of data on large clusters. Hadoop has three important components:\n",
    "\n",
    "**Hadoop Distributed File System** - A distributed file-system that stores data and facilitates the sharing of data between different machines in a Hadoop cluster (group of machines).\n",
    "\n",
    "**Hadoop YARN** - A platform for managing the computing resources available to Hadoop, notably performing the task of scheduling jobs to run on other machines.\n",
    "\n",
    "**Hadoop MapReduce** - Support for the MapReduce programming model for large-scale data processing\n",
    "\n",
    "All of these are already set up and (mostly) configured in the Virtual Machine, though this tutorial will walk you through starting and using these tools.\n",
    "\n",
    "## Firstly: Start a new terminal\n",
    "In addition to running Notebooks, Jupyter is also capable of running a terminal, an interactive text-based interface to the Virtual machine. On the main menu on the `Home` page, you can start a new terminal by clicking on `New` -> `Terminal`.  \n",
    "\n",
    "We'll be using this to run some of the commands necessary to configure Hadoop. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hadoop Modes of Operation\n",
    "\n",
    "Hadoop has three main modes of operation:\n",
    "\n",
    "**Standalone Mode** - This is the default mode used by Hadoop. It's localised to the current machine, and doesn't use HDFS, instead reading files from the local filesystem. It's primarily used for debugging.\n",
    "\n",
    "**Pseudo-Distributed Mode** - This is where Hadoop uses a cluster consisting of only a single machine, with every Hadoop daemon (a type of program that sits there doing work in that background) running on that machine. This is mainly used for testing the Hadoop setup. \n",
    "\n",
    "**Fully-Distributed Mode** - This is where data and processing is split between multiple machines. This enables Hadoop to horizontally scale and leverage the resources of multiple machines. This is the main mode used by Hadoop in production.\n",
    "\n",
    "In this tutorial, we'll only be using Standalone and Pseudo-Distributed modes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## MapReduce\n",
    "\n",
    "MapReduce is a programming model used by Hadoop to process large amounts of data in parallel. It accepts input data in the form of a set of key-value pairs <key1, value1>. It divides this set into individual chunks and assigns them as tasks to be processed on individual machines. It works in two phases: A Map phase and a Reduce phase.\n",
    "\n",
    "The Map phase takes these key-value pairs in the form <key1, value1> and maps (processes them) into other, intermediate key-value pairs <key2, value2>.\n",
    "\n",
    "These pairs are then sorted by their key, and passed into the Reduce phase.\n",
    "\n",
    "The Reduce phase takes these keys and produces a third (smaller) set of keys, combining the elements from the intermediate pairs that share a common key.\n",
    "\n",
    "In summary:\n",
    "\n",
    "**<key1, value1>** is *mapped* to **<key2, value2>** which is *reduced* to a smaller set of **<key3, value3>**.\n",
    "\n",
    "Don't worry if it's all a bit abstract - there'll be examples in the rest of the tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing data\n",
    "\n",
    "The first thing we're going to do, is download some data.  We will store this on our VMs, but could represent data which is remote, or in a datacentre somewhere.  Run the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  YouTube-Spam-Collection-v1.zip\n",
      "  inflating: Youtube01-Psy.csv       \n",
      "  inflating: __MACOSX/._Youtube01-Psy.csv  \n",
      "  inflating: Youtube02-KatyPerry.csv  \n",
      "  inflating: __MACOSX/._Youtube02-KatyPerry.csv  \n",
      "  inflating: Youtube03-LMFAO.csv     \n",
      "  inflating: __MACOSX/._Youtube03-LMFAO.csv  \n",
      "  inflating: Youtube04-Eminem.csv    \n",
      "  inflating: __MACOSX/._Youtube04-Eminem.csv  \n",
      "  inflating: Youtube05-Shakira.csv   \n",
      "  inflating: __MACOSX/._Youtube05-Shakira.csv  \n",
      "-rw-r--r-- 1 comp6235 comp6235 57K Mar 26  2017 Youtube01-Psy.csv\n",
      "-rw-r--r-- 1 comp6235 comp6235 63K Mar 26  2017 Youtube02-KatyPerry.csv\n",
      "-rw-r--r-- 1 comp6235 comp6235 63K Mar 26  2017 Youtube03-LMFAO.csv\n",
      "-rw-r--r-- 1 comp6235 comp6235 81K Mar 26  2017 Youtube04-Eminem.csv\n",
      "-rw-r--r-- 1 comp6235 comp6235 72K Mar 26  2017 Youtube05-Shakira.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2018-11-24 12:11:10--  https://archive.ics.uci.edu/ml/machine-learning-databases/00380/YouTube-Spam-Collection-v1.zip\n",
      "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.249\n",
      "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.249|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 163567 (160K) [application/zip]\n",
      "Saving to: ‘YouTube-Spam-Collection-v1.zip’\n",
      "\n",
      "     0K .......... .......... .......... .......... .......... 31% 43.5K 3s\n",
      "    50K .......... .......... .......... .......... .......... 62% 67.4K 1s\n",
      "   100K .......... .......... .......... .......... .......... 93% 79.7K 0s\n",
      "   150K .........                                             100% 30.1M=2.5s\n",
      "\n",
      "2018-11-24 12:11:14 (63.4 KB/s) - ‘YouTube-Spam-Collection-v1.zip’ saved [163567/163567]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "wget https://archive.ics.uci.edu/ml/machine-learning-databases/00380/YouTube-Spam-Collection-v1.zip \\\n",
    "-O YouTube-Spam-Collection-v1.zip\n",
    "\n",
    "unzip -o YouTube-Spam-Collection-v1.zip\n",
    "\n",
    "ls -lh *.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having downloaded the data, we want to be able to do a MapReduce task on it.  To do this, we will use the Hadoop Streaming API, which allows us to write Python code rather than the usual Java.  \n",
    "\n",
    "When we call the Hadoop process, we pass two Python files to the command - one which maps, and one which reduces.\n",
    "\n",
    "First, let's look at the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMMENT_ID,AUTHOR,DATE,CONTENT,CLASS\n",
      "z12rwfnyyrbsefonb232i5ehdxzkjzjs2,Lisa Wellas,,+447935454150 lovely girl talk to me xxx﻿,1\n",
      "z130wpnwwnyuetxcn23xf5k5ynmkdpjrj04,jason graham,2015-05-29T02:26:10.652000,I always end up coming back to this song<br />﻿,0\n",
      "z13vsfqirtavjvu0t22ezrgzyorwxhpf3,Ajkal Khan,,\"my sister just received over 6,500 new <a rel=\"\"nofollow\"\" class=\"\"ot-hashtag\"\" href=\"\"https://plus.google.com/s/%23active\"\">#active</a> youtube views Right now. The only thing she used was pimpmyviews. com﻿\",1\n",
      "z12wjzc4eprnvja4304cgbbizuved35wxcs,Dakota Taylor,2015-05-29T02:13:07.810000,Cool﻿,0\n",
      "z13xjfr42z3uxdz2223gx5rrzs3dt5hna,Jihad Naser,,Hello I&#39;am from Palastine﻿,1\n",
      "z133yfmjdur4dvyjr04ceh2osl2fvngrqi4,Darrion Johnson,2015-05-29T01:27:30.360000,Wow this video almost has a billion views! Didn&#39;t know it was so popular ﻿,0\n",
      "z12zgrw5furdsn0sc233hfwavnznyhicq,kyeman13,,Go check out my rapping video called Four Wheels please ❤️﻿,1\n",
      "z12vxdzzds2kzzrzq04cdjc4ozq2szuyl5o,Damax,2015-05-29T00:41:22.426000,Almost 1 billion﻿,0\n",
      "z12gxdortqzwhhqas04cfjrwituzghb5tvk0k,Muhammad Asim Mansha,,Aslamu Lykum... From Pakistan﻿,1\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "head -n 10 Youtube04-Eminem.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check that Hadoop is running\n",
    "\n",
    "The next thing to do is to check that we have Hadoop installed and running.  Open a terminal, and type in: \n",
    "\n",
    "    hadoop version\n",
    "    \n",
    "This should show you that the version you have is Hadoop 2.8.5.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Word counting\n",
    "\n",
    "Now we have our CSV files, let's get started processing them.\n",
    "\n",
    "The first thing we want to do is to set up a MapReduce function which will allow us to count the number of each individual word from the `comment` field of the file.\n",
    "\n",
    "The streaming API uses streams, which means that the information passed in to the map process is information from the output of one of our CSV files, and the data is then passed between the map and reduce process is output which is printed to stdout.\n",
    "\n",
    "The streaming API provides us a stream of data to the program's \"standard input\", more commonly called \"stdin\". In this case, we'll get each of the lines of our CSV file as the input to the mapper. The mapper will then then process this, and put it to \"standard output\" or \"stdout\". This will then be used as the input to the reduce process, and so on.\n",
    "\n",
    "It helps to first think of what the inputs and outputs of each stage of the process are. For word counting, we could do something like this:\n",
    "\n",
    "**Line of CSV Data** is mapped to **<Word, 1>** is mapped to **<Word, Count>**\n",
    "\n",
    "By default, the Streaming API uses a *tab character* as a seperator between the key and the value. The output of your map function might look something like:\n",
    "\n",
    "    \"Banana\\t1\" or \"Banana    1\"\n",
    "\n",
    "Some code has been provided to you below, including the libraries used in our answer. However, other solutions are possible that do not use these libraries.\n",
    "\n",
    "The cells below use the %%writefile magic keyword to write their contents to a file, instead of executing them.\n",
    "If you wish to execute them, comment this out with a `#`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/env python\n",
    "# MAPPER\n",
    "\n",
    "import csv\n",
    "import sys\n",
    "import re\n",
    "\n",
    "lines = sys.stdin.readlines()\n",
    "\n",
    "csvreader = csv.reader(lines)\n",
    "# YOUR CODE GOES BELOW\n",
    "\n",
    "# Create a list of ONLY the comments using a list comprehension\n",
    "comments = [row[3] for row in csvreader]  #取出第四行\n",
    "\n",
    "# Iterate over each of the comments\n",
    "for comment in comments:\n",
    "    # Split the comment string into words, using every whitespace character as a divider. \n",
    "    tokens = re.split(\"\\s\", comment)\n",
    "    for token in tokens:\n",
    "        #Print the key, value pair <token, 1>\n",
    "        print(token + \"\\t1\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/env python\n",
    "# REDUCER\n",
    "\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "# Keep simple example in for now, switch to stdin later\n",
    "\n",
    "input_pairs = [\n",
    "    '+447935454150\t1',\n",
    "    'lovely\t1',\n",
    "    'lovely\t1',\n",
    "    'girl\t1',\n",
    "    'talk\t1',\n",
    "    'to\t1',\n",
    "    'me\t1'\n",
    "    #'xxx\t1',\n",
    "     #Add an extra one to test that it works\n",
    "    #'to\\t1'\n",
    "]   #这个列表先作为测试\n",
    "# Once we test this with streams, we can uncomment this next line\n",
    "input_pairs = sys.stdin.readlines()\n",
    "\n",
    "# YOUR CODE GOES BELOW\n",
    "\n",
    "# Create a default dictionary. \n",
    "# This is a key-value store (dictionary) which returns a default value if the key hasn't been added.\n",
    "# Here, we use it to store <word, count> pairs.\n",
    "accumulator = defaultdict(lambda: 0)  #定义一个缺省的字典\n",
    "\n",
    "for row in input_pairs:  #轮询列表中的每行\n",
    "    # Split the line into our key value pair.\n",
    "    key_value_pair = row.split(\"\\t\", 1)  #字符串的split函数,第二个参数1表示分成1+1块\n",
    "    \n",
    "    # If we don't have a pair, ignore the line, as something has gone wrong.\n",
    "    if len(key_value_pair) != 2:\n",
    "        continue\n",
    "        \n",
    "    word = key_value_pair[0]  #该行的文字\n",
    "    # Strip removes whitespace at the start and end of a string. In this case, making sure we have just a number.\n",
    "    # We also convert it to an integer here.\n",
    "    count = int(key_value_pair[1].strip())  #strip函数去除字符串的首尾空格\n",
    "    \n",
    "    # Retrieve the count of that word we've seen so far, add to it, then store the result.\n",
    "    accumulator[word] = accumulator[word] + count #word作为字典的key,其后的数字作为value\n",
    "    \n",
    "for (key, value) in accumulator.items():\n",
    "    print(key + \"\\t\" + str(value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure the above files have been written to two files: `mapper.py` and `reducer.py`. The easiest way to do this is make sure the `%%writefile mapper/reducer.py` lines are uncommented, then run the cell.\n",
    "\n",
    "Since the mapper and reducer accept a stream into their stdin and output to stdout, we can test whether the scripts above work as a pipeline without using Hadoop!\n",
    "\n",
    "The below command reads the .csv file, then pipes the output to `mapper.py`'s stdin. The mapper's output is then piped to `reducer.py`, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"  File \"./mapper.py\", line 14\n",
      "SyntaxError: Non-ASCII character '\\xe5' in file ./mapper.py on line 14, but no encoding declared; see http://python.org/dev/peps/pep-0263/ for details\n",
      "cat: write error./reducer.py\", line 28\n",
      "SyntaxError: Non-ASCII character '\\xe5' in file ./reducer.py on line 28, but no encoding declared; see http://python.org/dev/peps/pep-0263/ for details\n",
      ": Broken pipe\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "chmod a+x mapper.py reducer.py\n",
    "cat Youtube04-Eminem.csv | ./mapper.py | ./reducer.py | sort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've tested our pipeline works, it's time to integrate it into hadoop. The below commands clear the output folder, ensure Hadoop is in standalone mode, then run our pipeline.\n",
    "\n",
    "The parameters are as follows:\n",
    "\n",
    "`-files` - Ensures these files are provided to every machine in our cluster.\n",
    "\n",
    "`-input` - The data sources to be passed to the pipeline.\n",
    "\n",
    "`-mapper` - The mapper to use.\n",
    "\n",
    "`-reducer` - The reducer to use.\n",
    "\n",
    "`-output` - The output folder.\n",
    "\n",
    "Test out your pipeline by running the command below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hadoop switched to standalone mode.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenJDK Server VM warning: You have loaded library /opt/hadoop-2.8.5/lib/native/libhadoop.so.1.0.0 which might have disabled stack guard. The VM will try to fix the stack guard now.\n",
      "It's highly recommended that you fix the library with 'execstack -c <libfile>', or link it with '-z noexecstack'.\n",
      "18/11/24 12:12:07 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "18/11/24 12:12:08 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "18/11/24 12:12:08 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "18/11/24 12:12:08 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "18/11/24 12:12:09 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "18/11/24 12:12:09 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "18/11/24 12:12:10 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local818906407_0001\n",
      "18/11/24 12:12:11 INFO mapred.LocalDistributedCacheManager: Localized file:/home/comp6235/Notebooks/mapper.py as file:/tmp/hadoop-comp6235/mapred/local/1543061530543/mapper.py\n",
      "18/11/24 12:12:11 INFO mapred.LocalDistributedCacheManager: Localized file:/home/comp6235/Notebooks/reducer.py as file:/tmp/hadoop-comp6235/mapred/local/1543061530544/reducer.py\n",
      "18/11/24 12:12:11 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "18/11/24 12:12:11 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "18/11/24 12:12:11 INFO mapreduce.Job: Running job: job_local818906407_0001\n",
      "18/11/24 12:12:11 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "18/11/24 12:12:11 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "18/11/24 12:12:11 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "18/11/24 12:12:11 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "18/11/24 12:12:11 INFO mapred.LocalJobRunner: Starting task: attempt_local818906407_0001_m_000000_0\n",
      "18/11/24 12:12:12 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "18/11/24 12:12:12 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "18/11/24 12:12:12 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "18/11/24 12:12:12 INFO mapred.MapTask: Processing split: file:/home/comp6235/Notebooks/Youtube04-Eminem.csv:0+82896\n",
      "18/11/24 12:12:12 INFO mapred.MapTask: numReduceTasks: 1\n",
      "18/11/24 12:12:12 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "18/11/24 12:12:12 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "18/11/24 12:12:12 INFO mapred.MapTask: soft limit at 83886080\n",
      "18/11/24 12:12:12 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "18/11/24 12:12:12 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "18/11/24 12:12:12 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "18/11/24 12:12:12 INFO streaming.PipeMapRed: PipeMapRed exec [/home/comp6235/Notebooks/././mapper.py]\n",
      "18/11/24 12:12:12 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "18/11/24 12:12:12 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "18/11/24 12:12:12 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "18/11/24 12:12:12 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "18/11/24 12:12:12 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "18/11/24 12:12:12 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "18/11/24 12:12:12 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "18/11/24 12:12:12 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "18/11/24 12:12:12 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "18/11/24 12:12:12 INFO mapreduce.Job: Job job_local818906407_0001 running in uber mode : false\n",
      "18/11/24 12:12:12 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "18/11/24 12:12:12 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "18/11/24 12:12:12 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "18/11/24 12:12:12 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "18/11/24 12:12:12 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "18/11/24 12:12:12 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "18/11/24 12:12:12 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "18/11/24 12:12:12 INFO streaming.PipeMapRed: Records R/W=454/1\n",
      "18/11/24 12:12:13 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "18/11/24 12:12:13 INFO streaming.PipeMapRed: mapRedFinished\n",
      "18/11/24 12:12:13 INFO mapred.LocalJobRunner: \n",
      "18/11/24 12:12:13 INFO mapred.MapTask: Starting flush of map output\n",
      "18/11/24 12:12:13 INFO mapred.MapTask: Spilling map output\n",
      "18/11/24 12:12:13 INFO mapred.MapTask: bufstart = 0; bufend = 70751; bufvoid = 104857600\n",
      "18/11/24 12:12:13 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26177480(104709920); length = 36917/6553600\n",
      "18/11/24 12:12:13 INFO mapred.MapTask: Finished spill 0\n",
      "18/11/24 12:12:13 INFO mapred.Task: Task:attempt_local818906407_0001_m_000000_0 is done. And is in the process of committing\n",
      "18/11/24 12:12:13 INFO mapred.LocalJobRunner: Records R/W=454/1\n",
      "18/11/24 12:12:13 INFO mapred.Task: Task 'attempt_local818906407_0001_m_000000_0' done.\n",
      "18/11/24 12:12:13 INFO mapred.Task: Final Counters for attempt_local818906407_0001_m_000000_0: Counters: 17\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=218861\n",
      "\t\tFILE: Number of bytes written=606793\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=454\n",
      "\t\tMap output records=9230\n",
      "\t\tMap output bytes=70751\n",
      "\t\tMap output materialized bytes=89219\n",
      "\t\tInput split bytes=102\n",
      "\t\tCombine input records=0\n",
      "\t\tSpilled Records=9230\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=57\n",
      "\t\tTotal committed heap usage (bytes)=137433088\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=82896\n",
      "18/11/24 12:12:13 INFO mapred.LocalJobRunner: Finishing task: attempt_local818906407_0001_m_000000_0\n",
      "18/11/24 12:12:13 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "18/11/24 12:12:13 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "18/11/24 12:12:13 INFO mapred.LocalJobRunner: Starting task: attempt_local818906407_0001_r_000000_0\n",
      "18/11/24 12:12:13 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "18/11/24 12:12:13 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "18/11/24 12:12:13 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "18/11/24 12:12:13 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@199f6d4\n",
      "18/11/24 12:12:13 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=363285696, maxSingleShuffleLimit=90821424, mergeThreshold=239768576, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "18/11/24 12:12:13 INFO reduce.EventFetcher: attempt_local818906407_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "18/11/24 12:12:13 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "18/11/24 12:12:13 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local818906407_0001_m_000000_0 decomp: 89215 len: 89219 to MEMORY\n",
      "18/11/24 12:12:13 INFO reduce.InMemoryMapOutput: Read 89215 bytes from map-output for attempt_local818906407_0001_m_000000_0\n",
      "18/11/24 12:12:13 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 89215, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->89215\n",
      "18/11/24 12:12:13 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "18/11/24 12:12:13 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "18/11/24 12:12:13 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "18/11/24 12:12:13 INFO mapred.Merger: Merging 1 sorted segments\n",
      "18/11/24 12:12:13 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 89212 bytes\n",
      "18/11/24 12:12:13 INFO reduce.MergeManagerImpl: Merged 1 segments, 89215 bytes to disk to satisfy reduce memory limit\n",
      "18/11/24 12:12:13 INFO reduce.MergeManagerImpl: Merging 1 files, 89219 bytes from disk\n",
      "18/11/24 12:12:13 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "18/11/24 12:12:13 INFO mapred.Merger: Merging 1 sorted segments\n",
      "18/11/24 12:12:13 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 89212 bytes\n",
      "18/11/24 12:12:13 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "18/11/24 12:12:13 INFO streaming.PipeMapRed: PipeMapRed exec [/home/comp6235/Notebooks/././reducer.py]\n",
      "18/11/24 12:12:13 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "18/11/24 12:12:13 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "18/11/24 12:12:14 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "18/11/24 12:12:14 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "18/11/24 12:12:14 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "18/11/24 12:12:14 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "18/11/24 12:12:14 INFO streaming.PipeMapRed: Records R/W=6957/1\n",
      "18/11/24 12:12:14 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "18/11/24 12:12:14 WARN streaming.PipeMapRed: java.io.IOException: Stream closed\n",
      "18/11/24 12:12:14 INFO streaming.PipeMapRed: mapRedFinished\n",
      "18/11/24 12:12:14 INFO mapred.Task: Task:attempt_local818906407_0001_r_000000_0 is done. And is in the process of committing\n",
      "18/11/24 12:12:14 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "18/11/24 12:12:14 INFO mapred.Task: Task attempt_local818906407_0001_r_000000_0 is allowed to commit now\n",
      "18/11/24 12:12:14 INFO output.FileOutputCommitter: Saved output of task 'attempt_local818906407_0001_r_000000_0' to file:/home/comp6235/Notebooks/output/_temporary/0/task_local818906407_0001_r_000000\n",
      "18/11/24 12:12:14 INFO mapred.LocalJobRunner: Records R/W=6957/1 > reduce\n",
      "18/11/24 12:12:14 INFO mapred.Task: Task 'attempt_local818906407_0001_r_000000_0' done.\n",
      "18/11/24 12:12:14 INFO mapred.Task: Final Counters for attempt_local818906407_0001_r_000000_0: Counters: 24\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=397331\n",
      "\t\tFILE: Number of bytes written=696073\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=2516\n",
      "\t\tReduce shuffle bytes=89219\n",
      "\t\tReduce input records=9230\n",
      "\t\tReduce output records=6\n",
      "\t\tSpilled Records=9230\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=11\n",
      "\t\tTotal committed heap usage (bytes)=137433088\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=61\n",
      "18/11/24 12:12:14 INFO mapred.LocalJobRunner: Finishing task: attempt_local818906407_0001_r_000000_0\n",
      "18/11/24 12:12:14 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "18/11/24 12:12:14 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "18/11/24 12:12:14 INFO mapreduce.Job: Job job_local818906407_0001 completed successfully\n",
      "18/11/24 12:12:14 INFO mapreduce.Job: Counters: 30\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=616192\n",
      "\t\tFILE: Number of bytes written=1302866\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=454\n",
      "\t\tMap output records=9230\n",
      "\t\tMap output bytes=70751\n",
      "\t\tMap output materialized bytes=89219\n",
      "\t\tInput split bytes=102\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=2516\n",
      "\t\tReduce shuffle bytes=89219\n",
      "\t\tReduce input records=9230\n",
      "\t\tReduce output records=6\n",
      "\t\tSpilled Records=18460\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=68\n",
      "\t\tTotal committed heap usage (bytes)=274866176\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=82896\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=61\n",
      "18/11/24 12:12:14 INFO streaming.StreamJob: Output directory: output\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "rm -rf output\n",
    "\n",
    "hadoop-standalone-mode.sh\n",
    "\n",
    "hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-*.jar \\\n",
    "-files mapper.py,reducer.py \\\n",
    "-input Youtube04-Eminem.csv \\\n",
    "-mapper ./mapper.py \\\n",
    "-reducer ./reducer.py \\\n",
    "-output output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up HDFS\n",
    "\n",
    "Now you've had a chance to use Hadoop in standalone mode, it's time we set it to pseudo-distributed mode and set up HDFS.\n",
    "\n",
    "To speed things up, some commands have been provided to easily configure Hadoop. Start by running in your terminal:\n",
    "\n",
    "    hadoop-pseudo-distributed-mode.sh\n",
    "    \n",
    "If you're curious how this works, feel free to have a read of the code, you'll find it in `~/vm_creation/scripts`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should now have HDFS configured for pseudo-distributed mode.  We will now need to create a disk for HDFS, which will use the configurations we just set:\n",
    "\n",
    "    hdfs namenode -format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting services\n",
    "\n",
    "Now we need to start the different services and we can get to work!  Run the following command in the terminal to start the HDFS:\n",
    "\n",
    "    start-dfs.sh\n",
    "\n",
    "You'll also need to start YARN in order to run any MapReduce jobs, so let's do that now:\n",
    "\n",
    "    start-yarn.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see what this has left you with, you can see the processes which are running on the JVM by running the `jps` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "jps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see something similar to the following:\n",
    "\n",
    "```\n",
    "XXXX ResourceManager\n",
    "XXXX SecondaryNameNode\n",
    "XXXX NameNode\n",
    "XXXX DataNode\n",
    "XXXX NodeManager\n",
    "```\n",
    "\n",
    "        If any of these aren't running, double check that you've run all of the above commands. If any are still missing, you may encounter errors later, so please contact one of the demonstrators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a HDFS disk, and the appropriate Hadoop services running in pseudo-distributed mode, we can start to import the data into the new HDFS filesystem and run the MapReduce task there. \n",
    "\n",
    "Fully-distributed mode runs on the exact same principles described below, so we could apply MapReduce over various machines.\n",
    "\n",
    "In summary, we need to: \n",
    "\n",
    "* Create a directory for the input\n",
    "* Import the data from the local file to the HDFS datanode\n",
    "* Run the MapReduce job\n",
    "* View the output\n",
    "\n",
    "The Commands on HDFS are similar to standard linux CLI commands, except for the fact that they are prefixed by either `hadoop fs` or `hdfs dfs`.\n",
    "\n",
    "The `hadoop fs` command is more general, as it can cope with different types of filesystem, such as the one on the local disk.  As such, this is a better choice to use for commands relating solely to HDFS.\n",
    "\n",
    "The command to create a directory is `-mkdir`.  Create a directory `/input` on the HDFS system.  Use the `hdfs dfs` command below to achieve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# YOUR CODE HERE\n",
    "hdfs dfs -mkdir /input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to import our data into HDFS. Here, we are dealing with two different filesystems: the local system and the HDFS node so we will use `hadoop fs`, with the `-copyFromLocal` command. This command copies files from the local filesystem to HDFS, accepting two arguments: file source and destination.\n",
    "\n",
    "HDFS filesystems are defined by a URI prefixed by `hdfs://`, and the `hdfs dfs` and `hadoop fs` commands will normally expect to see them.\n",
    "\n",
    "If they are not specified, the default location of the filesystem is specified in `core-site.xml`, which is one of the config files we imported earlier.  The value can be seen from the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "cat $HADOOP_HOME/etc/hadoop/core-site.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are interested in learning more about the configuration options we have specified for Hadoop, check out the documentation for Hadoop, as well as the `~/vm_creation/hadoop` folder.\n",
    "\n",
    "For the `-copyFromLocal` we can either specify `hdfs://localhost:9000/` or leave it out, instead using `hdfs:///`. For example, `hdfs://localhost:9000/input` and `hdfs:///input` refer to the same location.\n",
    "\n",
    "The local file can be specified with a relative command, leaving the import command as one of the following two.  Pick one and execute it in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# With fully specified URI\n",
    "hadoop fs -copyFromLocal *.csv hdfs://localhost:9000/input\n",
    "\n",
    "# Explicit HDFS, but with the default host\n",
    "# hadoop fs -copyFromLocal *.csv hdfs:///input\n",
    "\n",
    "# Implied URI based on default\n",
    "# hadoop fs -copyFromLocal *.csv /input\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll check that the files have been successfully imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "hadoop fs -ls /input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the same for the `mapper.py` and `reducer.py` files we created for the MapReduce task earlier, keeping those in the `input` directory as well.\n",
    "\n",
    "You may need to add `-p` and `-f` as options. These options preserve file permissions, and force the new files to overwrite any existing files, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# YOUR CODE HERE\n",
    "hdfs dfs -copyFromLocal -p -f mapper.py reducer.py /input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we run the hadoop command again, this time sourcing our files from HDFS instead of the local filesystem.\n",
    "\n",
    "Note: If you run this command more than once, Hadoop will throw an error due to the output directory already existing. You may need to erase the existing directory or output to one with a different name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-*.jar \\\n",
    "-files hdfs:///input/mapper.py,hdfs:///input/reducer.py \\\n",
    "-input hdfs:///input/Youtube04-Eminem.csv \\\n",
    "-mapper ./mapper.py \\\n",
    "-reducer ./reducer.py \\\n",
    "-output hdfs://localhost:9000/output_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, write a command to view the files listed in the `/output_2` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# YOUR CODE HERE\n",
    "hdfs dfs -ls /output_16\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `_SUCCCESS` file indicates that the job was a success, which is good.  The other file, `part-00000` contains the result.  Write code in the cell below to get the output (from HDFS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# YOUR CODE HERE\n",
    "hdfs dfs -cat /output_16/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can include multiple `-input` parameters to operate on more than one file.  Update the streaming command above to include all 5 files in the cell below.  Make sure you include a new output directory!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Update this command to include multiple files\n",
    "hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-*.jar \\\n",
    "-files hdfs:///input/mapper.py,hdfs:///input/reducer.py \\\n",
    "-input hdfs://localhost:9000/input/Youtube04-Eminem.csv \\\n",
    "-mapper ./mapper.py \\\n",
    "-reducer ./reducer.py \\\n",
    "-output hdfs://localhost:9000/output_8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, you have started to use Hadoop and HDFS.  You created a MapReduce task using the Hadoop streaming framework, and then set up your Hadoop instance to work in pseudo-distributed mode.\n",
    "\n",
    "### Where next?\n",
    "You might want to look at Mahout, and try and put the data here into a format like the one in the [video](https://www.youtube.com/watch?v=TWl6AIZIVps) which can be used to train a naive Bayes algorithm to classify the spam data.  Alternatively, you can try and find the [SpamAssassin dataset](http://csmining.org/index.php/spam-email-datasets-.html) and import that yourself.\n",
    "\n",
    "Hadoop over multiple machines is difficult to configure.  You might try and look at systems which assist you to do this, such as Ambari or Cloudera, and try to do these yourself.  If you don't have multiple computers available, to practice, give the [Caochong](https://github.com/weiqingy/caochong) library a try which sets up Hadoop in Docker containers on a single computer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
